#Data Base 접속 정보
NEO4J_URI=bolt://127.0.0.1:7687
NEO4J_INSTANCE=CSADB
NEO4J_DATABASE=csadb01
NEO4J_USER=csauser
NEO4J_PASSWORD=your-password
NEO4J_POOL_SIZE=10
NEO4J_BATCH_SIZE=100

JAVA_SOURCE_FOLDER = target_src/car-center-devlab
DB_SCRIPT_FOLDER = target_src/car-center-devlab/src/main/resources/db/prod
USE_NEO4J_BEAN_RESOLVER=true
# 스트리밍 파싱 모드 활성화 (병렬처리 + 메모리 효율)
USE_STREAMING_PARSE=true

# Java 파싱 병렬 워커 수 (기본값: 8)
# CPU 코어 수에 맞게 조정 (권장: 4-16)
JAVA_PARSE_WORKERS=8

# LOG_LEVEL can be: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# 출력 디렉터리 설정 (선택사항)
SEQUENCE_DIAGRAM_OUTPUT_DIR=output/sequence-diagram
CRUD_MATRIX_OUTPUT_DIR=output/crud-matrix
CLASS_SPEC_OUTPUT_DIR=output/class-spec
IMPACT_ANALYSIS_OUTPUT_DIR=output/impact-analysis

# AI를 적용해서 Class, Method, SQL document 문장 생성(default = false)
AI_USE_ANALYSIS=false

# AI Enrichment 동시 요청 수 (기본값: 10)
# ai-enrich 명령어에서 동시에 처리할 노드 수
# 로컬 LLM (LM Studio): 10-20 권장
# 클라우드 API (Google, OpenAI): 5-10 권장
# Rate Limit 발생 시 이 값을 줄여서 재시도
CONCURRENT_AI_REQUESTS=10

# AI Enrichment 배치 크기 (deprecated: 하위 호환성을 위해 유지)
# 대신 CONCURRENT_AI_REQUESTS 사용을 권장합니다
AI_ENRICHMENT_BATCH_SIZE=50

# AI Provider 설정 (google, groq, lmstudio, openai 중 선택)
AI_PROVIDER=google

# Google Gemini 설정
GOOGLE_API_KEY=<your-api-key>
GEMINI_MODEL_NAME=gemini-1.5-flash
GOOGLE_CENAI_USE_VERTEXAI=FALSE

# Groq 설정
GROQ_API_KEY=<your-api-key>
GROQ_MODEL_NAME=qwen/qwen3-32b

# LM Studio 설정
LMSTUDIO_BASE_URL=http://localhost:1234/v1
LMSTUDIO_QWEN_MODEL_NAME=lm_studio/qwen/qwen3-8b

# OpenAI 설정
OPENAI_API_KEY=ollama
OPENAI_MODEL_NAME=gpt-oss:20b
#OPENAI_MODEL_NAME=qwen3:30b
OPENAI_BASE_URL=http://devlab.skax.co.kr/ollama/v1